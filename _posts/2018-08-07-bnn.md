--- 
 layout: post
 category: [blog,pgm] 
 title:  Bayesian Neural Networks
 tags: [Deep learning, Bayesian Neural Networks]
 mathjax: true
 elements:
- GraphicalModel

---

Bayesian Neural Networks uses Proabibilitic Model along with Neural Networks.



* Probabilistic Programming Libraries:
	* PyMC3, Edward, Stan
* Bayes Law <br>
	$$ Posterior = \frac {likelihood * prior}{Normalizing Constant }$$

<!-- IN N Dimensional simplex noise, the squared kernel summation radius $r^2$ is $\frac 1 2$
for all values of N. This is because the edge length of the N-simplex $s = \sqrt {\frac {N} {N + 1}}$
divides out of the N-simplex height $h = s \sqrt {\frac {N + 1} {2N}}$.
The kerel summation radius $r$ is equal to the N-simplex height $h$.

$$ r = h = \sqrt{\frac {1} {2}} = \sqrt{\frac {N} {N+1}} \sqrt{\frac {N+1} {2N}} $$
 --><br>
#### References:

1. [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/abs/1506.02142  "Dropout").
2. [Loss-Calibrated Approximate Inference in Bayesian Neural Networks](https://arxiv.org/pdf/1805.03901.pdf  "Loss - Calibrated").
3. [Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding](https://arxiv.org/abs/1511.02680_)
4. [Weight Uncertainty in Neural Networks](https://arxiv.org/pdf/1505.05424)
5. [Study of Bayesian Neural Networks](https://arxiv.org/pdf/1801.07710.pdf)
