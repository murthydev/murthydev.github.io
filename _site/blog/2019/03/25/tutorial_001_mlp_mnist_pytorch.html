<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />

  <title>
    
      Pytorch MNIST simple CNN 001 &middot; Pramod Murthy
    
  </title>

  <!--  -->

<!--   <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45894387-2', 'auto');
  ga('send', 'pageview');
  </script>
 -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-45894387-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-45894387-3');
</script>




  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css" />
  

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface" />

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/favicon.png" />
<link rel="shortcut icon" href="/favicon.ico" />

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml" />

  <!-- Additional head bits without overriding original head -->
</head>


  <body class="post">

    <div id="sidebar">
  <p style="horizontal-align:center"><img align='center' src="/images/Pramod.png" alt="Profile Photo" width="65%"/></p>
  <header>
  <!-- <img src="/images/Pramod.png" alt="Profile Photo" height="30%" width="30%" style="horizontal-align:center"/> -->
    <!-- <img src='/images/me.jpg' width='55%' height='80%'/> -->
    <div class="site-title">
      <a href="/">
        
          <span class="back-arrow icon"><svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
  <path d="M0 0h24v24H0z" fill="none"/>
  <path d="M20 11H7.83l5.59-5.59L12 4l-8 8 8 8 1.41-1.41L7.83 13H20v-2z"/>
</svg></span>
        
        Pramod Murthy
      </a>
    </div>
    <p class="lead"> Researcher  </p>
  </header>

  <nav id="sidebar-nav-links">
  
    <a class="home-link "
        href="/">Home</a>
  
  
  
  


  
    
  

  
    
  

  
    
      <a class="page-link "
          href="/cv.html">Résumé</a>
    
  

  
    
  

  
    
  

  

  

  
    
      <a class="page-link "
          href="/projects.html">Projects</a>
    
  

  
    
      <a class="page-link "
          href="/publications.html">Research</a>
    
  

  
    
  

  
    
  

  
    
  

  


  


  
    
  

  
    
      <a class="category-link "
          href="/category/blog.html">Thought2Vec</a>
    
  

  
    
  

  
    
      <a class="category-link "
          href="/embedded-vision.html">Embedded Vision</a>
    
  

  
    
  

  

  

  
    
  

  
    
  

  
    
  

  
    
      <a class="category-link "
          href="/category/spiritual-musings.html">Musings</a>
    
  

  
    
  

  

  <!-- Optional additional links to insert in sidebar nav -->
</nav>

  <br/>
  <span>  Email: <a href="mailto:mail@pramodmurthy.com?subject=Hey ! You there ... &body=Hi Pramod, %0A%0A   I would like to get in touch with you ! %0A%0A -  YOUR_NAME ">mail@pramodmurthy.com</a></span>
  <nav id="sidebar-icon-links">
    
    <a id="github-link"
       class="icon" title="Github Project" aria-label="Github Project"
       href="https://github.com/murthydev">
      <svg version="1.1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 28"><path d="M12 2c6.625 0 12 5.375 12 12 0 5.297-3.437 9.797-8.203 11.391-0.609 0.109-0.828-0.266-0.828-0.578 0-0.391 0.016-1.687 0.016-3.297 0-1.125-0.375-1.844-0.812-2.219 2.672-0.297 5.484-1.313 5.484-5.922 0-1.313-0.469-2.375-1.234-3.219 0.125-0.313 0.531-1.531-0.125-3.187-1-0.313-3.297 1.234-3.297 1.234-0.953-0.266-1.984-0.406-3-0.406s-2.047 0.141-3 0.406c0 0-2.297-1.547-3.297-1.234-0.656 1.656-0.25 2.875-0.125 3.187-0.766 0.844-1.234 1.906-1.234 3.219 0 4.594 2.797 5.625 5.469 5.922-0.344 0.313-0.656 0.844-0.766 1.609-0.688 0.313-2.438 0.844-3.484-1-0.656-1.141-1.844-1.234-1.844-1.234-1.172-0.016-0.078 0.734-0.078 0.734 0.781 0.359 1.328 1.75 1.328 1.75 0.703 2.141 4.047 1.422 4.047 1.422 0 1 0.016 1.937 0.016 2.234 0 0.313-0.219 0.688-0.828 0.578-4.766-1.594-8.203-6.094-8.203-11.391 0-6.625 5.375-12 12-12zM4.547 19.234c0.031-0.063-0.016-0.141-0.109-0.187-0.094-0.031-0.172-0.016-0.203 0.031-0.031 0.063 0.016 0.141 0.109 0.187 0.078 0.047 0.172 0.031 0.203-0.031zM5.031 19.766c0.063-0.047 0.047-0.156-0.031-0.25-0.078-0.078-0.187-0.109-0.25-0.047-0.063 0.047-0.047 0.156 0.031 0.25 0.078 0.078 0.187 0.109 0.25 0.047zM5.5 20.469c0.078-0.063 0.078-0.187 0-0.297-0.063-0.109-0.187-0.156-0.266-0.094-0.078 0.047-0.078 0.172 0 0.281s0.203 0.156 0.266 0.109zM6.156 21.125c0.063-0.063 0.031-0.203-0.063-0.297-0.109-0.109-0.25-0.125-0.313-0.047-0.078 0.063-0.047 0.203 0.063 0.297 0.109 0.109 0.25 0.125 0.313 0.047zM7.047 21.516c0.031-0.094-0.063-0.203-0.203-0.25-0.125-0.031-0.266 0.016-0.297 0.109s0.063 0.203 0.203 0.234c0.125 0.047 0.266 0 0.297-0.094zM8.031 21.594c0-0.109-0.125-0.187-0.266-0.172-0.141 0-0.25 0.078-0.25 0.172 0 0.109 0.109 0.187 0.266 0.172 0.141 0 0.25-0.078 0.25-0.172zM8.937 21.438c-0.016-0.094-0.141-0.156-0.281-0.141-0.141 0.031-0.234 0.125-0.219 0.234 0.016 0.094 0.141 0.156 0.281 0.125s0.234-0.125 0.219-0.219z"></path>
</svg>

    </a>
  <!--  -->
  
  <a id="lnikedin-link"
       class="icon" title="LinkedIn" aria-label="LinkedIn"
       href="http://in.linkedin.com/in/pramodmurthy">
      <?xml version="1.0" standalone="yes"?>

<svg version="1.1" viewBox="0.0 0.0 37.79527559055118 37.79527559055118" fill="none" stroke="none" stroke-linecap="square" stroke-miterlimit="10" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><clipPath id="p.0"><path d="m0 0l37.795277 0l0 37.795277l-37.795277 0l0 -37.795277z" clip-rule="nonzero"></path></clipPath><g clip-path="url(#p.0)"><path fill="#ffffff" d="m0 0l37.795277 0l0 37.795277l-37.795277 0z" fill-rule="evenodd"></path><path fill="#000000" fill-opacity="0.0" d="m1.2883971 5.471258l35.212597 0l0 15.086615l-35.212597 0z" fill-rule="evenodd"></path><path fill="#000000" fill-opacity="0.0" d="m1.2883625 20.825613l35.2126 0l0 5.826771l-35.2126 0z" fill-rule="evenodd"></path><path fill="#000000" fill-opacity="0.0" d="m0 0l37.52231 0l0 37.52231l-37.52231 0z" fill-rule="evenodd"></path><g transform="matrix(0.03752230971128609 0.0 0.0 0.03752230971128609 0.0 0.0)"><clipPath id="p.1"><path d="m0 0l1000.0 0l0 1000.0l-1000.0 0z" clip-rule="evenodd"></path></clipPath><image clip-path="url(#p.1)" fill="#000" width="1000.0" height="1000.0" x="0.0" y="0.0" preserveAspectRatio="none" xlink:href="data:image/png;base64,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"></image></g></g></svg>


  </a>
  <a id="subscribe-link"
     class="icon" title="Subscribe" aria-label="Subscribe"
     href="/feed.xml">
    <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <circle cx="6.18" cy="17.82" r="2.18"/>
    <path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/>
</svg>
  </a>
  <!--    -->
  
  
  
  
    <a id="tags-link"
       class="icon"
       title="Tags" aria-label="Tags"
       href="/tags.html">
      <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
    </a>
  
  
    <a id="search-link"
       class="icon"
       title="Search" aria-label="Tags"
       href="/search.html">
      <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"/>
    <path d="M0 0h24v24H0z" fill="none"/>
</svg>
    </a>
  
   <!-- Optional additional links to insert for icons links -->
</nav>

 
  <br/>
  <p>
  &copy; 2022.
  <a href="/LICENSE.md">MIT License.</a>
</p>

  <!--     -->

</div>

    <main class="container">
      <header>
  <h1 class="post-title">Pytorch MNIST simple CNN 001</h1>
</header>
<div class="content">
  <div class="post-meta">
  <span class="post-date">25 Mar 2019</span>
  <span class="post-categories">
    
      &bull;

      
      
      

      
        <a href="/category/blog.html">
          blog
        </a>
      
    
  </span>
</div>
  

  <div class="post-body">
    <h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Pytorch-Tutorial-001" data-toc-modified-id="Pytorch-Tutorial-001-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Pytorch Tutorial 001</a></span><ul class="toc-item"><li><span><a href="#Defining-a-simple-convolutional-neural-network" data-toc-modified-id="Defining-a-simple-convolutional-neural-network-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Defining a simple convolutional neural network</a></span></li><li><span><a href="#Configuring-the-network-training-parameters" data-toc-modified-id="Configuring-the-network-training-parameters-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Configuring the network training parameters</a></span></li><li><span><a href="#Loading-the-datasets-using-pytorch-dataloaders" data-toc-modified-id="Loading-the-datasets-using-pytorch-dataloaders-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Loading the datasets using pytorch dataloaders</a></span></li><li><span><a href="#Setting-up-the-Optimizer-to-optimize-the-loss-function" data-toc-modified-id="Setting-up-the-Optimizer-to-optimize-the-loss-function-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>Setting up the Optimizer to optimize the loss function</a></span></li><li><span><a href="#Plotting-the-losses-and-Accuracy" data-toc-modified-id="Plotting-the-losses-and-Accuracy-1.5"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>Plotting the losses and Accuracy</a></span></li></ul></li></ul></div>

<h1 id="pytorch-tutorial-001">Pytorch Tutorial 001</h1>

<p>Lets train a simple CNN on MNIST dataset.</p>

<h2 id="defining-a-simple-convolutional-neural-network">Defining a simple convolutional neural network</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="mi">4</span><span class="o">*</span><span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="mi">4</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Net(
  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=800, out_features=500, bias=True)
  (fc2): Linear(in_features=500, out_features=10, bias=True)
)
</code></pre></div></div>

<h2 id="configuring-the-network-training-parameters">Configuring the network training parameters</h2>

<p>The configuration to train the network are as below</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_batch_size</span><span class="o">=</span><span class="mi">64</span>
<span class="n">test_batch_size</span><span class="o">=</span><span class="mi">1000</span>
<span class="n">epochs</span> <span class="o">=</span><span class="mi">30</span>
<span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">no_cuda</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">log_interval</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">save_model</span> <span class="o">=</span> <span class="bp">True</span>
</code></pre></div></div>

<h2 id="loading-the-datasets-using-pytorch-dataloaders">Loading the datasets using pytorch dataloaders</h2>

<p>We use two loader for training set and test set. You can also use an optional validation set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="n">use_cuda</span> <span class="o">=</span> <span class="bp">False</span><span class="c1">#not no_cuda and torch.cuda.is_available()
</span><span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

<span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'num_workers'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'pin_memory'</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="p">{}</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s">'data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                                  <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))])),</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s">'data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
                       <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span><span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))])),</span>

<span class="n">batch_size</span><span class="o">=</span><span class="n">test_batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Processing...
Done!
</code></pre></div></div>

<h2 id="setting-up-the-optimizer-to-optimize-the-loss-function">Setting up the Optimizer to optimize the loss function</h2>

<p>Using a simple stochastic gradient descent with nestrov’s momentum</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Math</span><span class="p">,</span> <span class="n">Latex</span>
<span class="n">display</span><span class="p">(</span><span class="n">Math</span><span class="p">(</span><span class="s">r'w:= w - \eta \sum_{i=1}^{n}\delta Q_{i}(w)/n'</span><span class="p">))</span>
</code></pre></div></div>

<script type="math/tex; mode=display">w:= w - \eta \sum_{i=1}^{n}\delta Q_{i}(w)/n</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span><span class="n">log_interval</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># in training loop:
</span>    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># zero the gradient buffers
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># Does the update
</span>        <span class="n">avg_loss</span><span class="o">+=</span><span class="n">F</span><span class="p">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Train Epoch: {} [{}/{} ({:.0f}%)]</span><span class="se">\t</span><span class="s">Loss: {:.6f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
                <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">),</span>
                <span class="mf">100.</span> <span class="o">*</span> <span class="n">batch_idx</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()))</span>
    <span class="n">avg_loss</span><span class="o">/=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_loss</span>

<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">F</span><span class="p">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">).</span><span class="n">item</span><span class="p">()</span> <span class="c1"># sum up batch loss
</span>            <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># get the index of the max log-probability
</span>            <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="p">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">)).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

    <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
        <span class="n">test_loss</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">),</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)))</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">test_loss</span><span class="p">,</span><span class="n">accuracy</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">trn_loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span><span class="n">log_interval</span><span class="p">)</span>
    <span class="n">test_loss</span><span class="p">,</span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
    <span class="n">train_losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">trn_loss</span><span class="p">)</span>
    <span class="n">test_losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)</span>
    <span class="n">accuracy_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>

<span class="k">if</span> <span class="p">(</span><span class="n">save_model</span><span class="p">):</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span><span class="s">"mnist_cnn.pt"</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train Epoch: 1 [0/60000 (0%)]	Loss: 2.300039

Test set: Average loss: 0.1018, Accuracy: 9663/10000 (97%)

Train Epoch: 2 [0/60000 (0%)]	Loss: 0.146140

Test set: Average loss: 0.0614, Accuracy: 9825/10000 (98%)

Train Epoch: 3 [0/60000 (0%)]	Loss: 0.052315

Test set: Average loss: 0.0562, Accuracy: 9813/10000 (98%)

Train Epoch: 4 [0/60000 (0%)]	Loss: 0.019015

Test set: Average loss: 0.0409, Accuracy: 9864/10000 (99%)

Train Epoch: 5 [0/60000 (0%)]	Loss: 0.010967

Test set: Average loss: 0.0380, Accuracy: 9874/10000 (99%)

Train Epoch: 6 [0/60000 (0%)]	Loss: 0.127716

Test set: Average loss: 0.0338, Accuracy: 9888/10000 (99%)

Train Epoch: 7 [0/60000 (0%)]	Loss: 0.027539

Test set: Average loss: 0.0345, Accuracy: 9873/10000 (99%)

Train Epoch: 8 [0/60000 (0%)]	Loss: 0.004211

Test set: Average loss: 0.0394, Accuracy: 9877/10000 (99%)

Train Epoch: 9 [0/60000 (0%)]	Loss: 0.090090

Test set: Average loss: 0.0292, Accuracy: 9910/10000 (99%)

Train Epoch: 10 [0/60000 (0%)]	Loss: 0.125310

Test set: Average loss: 0.0321, Accuracy: 9895/10000 (99%)

Train Epoch: 11 [0/60000 (0%)]	Loss: 0.008019

Test set: Average loss: 0.0303, Accuracy: 9896/10000 (99%)

Train Epoch: 12 [0/60000 (0%)]	Loss: 0.035579

Test set: Average loss: 0.0271, Accuracy: 9913/10000 (99%)

Train Epoch: 13 [0/60000 (0%)]	Loss: 0.018132

Test set: Average loss: 0.0284, Accuracy: 9913/10000 (99%)

Train Epoch: 14 [0/60000 (0%)]	Loss: 0.008078

Test set: Average loss: 0.0276, Accuracy: 9911/10000 (99%)

Train Epoch: 15 [0/60000 (0%)]	Loss: 0.007842

Test set: Average loss: 0.0258, Accuracy: 9914/10000 (99%)

Train Epoch: 16 [0/60000 (0%)]	Loss: 0.001491

Test set: Average loss: 0.0262, Accuracy: 9914/10000 (99%)

Train Epoch: 17 [0/60000 (0%)]	Loss: 0.001637

Test set: Average loss: 0.0248, Accuracy: 9924/10000 (99%)

Train Epoch: 18 [0/60000 (0%)]	Loss: 0.011664

Test set: Average loss: 0.0277, Accuracy: 9917/10000 (99%)

Train Epoch: 19 [0/60000 (0%)]	Loss: 0.023257

Test set: Average loss: 0.0297, Accuracy: 9906/10000 (99%)

Train Epoch: 20 [0/60000 (0%)]	Loss: 0.000937

Test set: Average loss: 0.0284, Accuracy: 9906/10000 (99%)

Train Epoch: 21 [0/60000 (0%)]	Loss: 0.002732

Test set: Average loss: 0.0246, Accuracy: 9920/10000 (99%)

Train Epoch: 22 [0/60000 (0%)]	Loss: 0.000329

Test set: Average loss: 0.0311, Accuracy: 9907/10000 (99%)

Train Epoch: 23 [0/60000 (0%)]	Loss: 0.002051

Test set: Average loss: 0.0264, Accuracy: 9912/10000 (99%)

Train Epoch: 24 [0/60000 (0%)]	Loss: 0.001115

Test set: Average loss: 0.0250, Accuracy: 9921/10000 (99%)

Train Epoch: 25 [0/60000 (0%)]	Loss: 0.001492

Test set: Average loss: 0.0288, Accuracy: 9915/10000 (99%)

Train Epoch: 26 [0/60000 (0%)]	Loss: 0.000727

Test set: Average loss: 0.0294, Accuracy: 9909/10000 (99%)

Train Epoch: 27 [0/60000 (0%)]	Loss: 0.008881

Test set: Average loss: 0.0284, Accuracy: 9916/10000 (99%)

Train Epoch: 28 [0/60000 (0%)]	Loss: 0.003079

Test set: Average loss: 0.0279, Accuracy: 9919/10000 (99%)

Train Epoch: 29 [0/60000 (0%)]	Loss: 0.000659

Test set: Average loss: 0.0291, Accuracy: 9924/10000 (99%)

Train Epoch: 30 [0/60000 (0%)]	Loss: 0.000261

Test set: Average loss: 0.0286, Accuracy: 9912/10000 (99%)
</code></pre></div></div>

<h2 id="plotting-the-losses-and-accuracy">Plotting the losses and Accuracy</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span><span class="s">'b'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_losses</span><span class="p">,</span><span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accuracy_list</span><span class="p">,</span><span class="s">'g'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/images/2019-03-25-001_mlp_mnist_pytorch_files/2019-03-25-001_mlp_mnist_pytorch_15_0.png" alt="img" /></p>

<p><img src="/images/2019-03-25-001_mlp_mnist_pytorch_files/2019-03-25-001_mlp_mnist_pytorch_15_1.png" alt="img" /></p>


    



<div class="post-tags">
  
    
    <a href="/tags.html#pytorch-tutorial">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">pytorch-tutorial</span>
    </a>
  
    
    <a href="/tags.html#deep-learning">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">deep learning</span>
    </a>
  
</div>
  </div>

  <!-- 
  <section class="comments">
    <h2>Comments</h2>
    
  <p>
    You are seeing this because your Disqus shortname is not properly set. To
    configure Disqus, you should edit your <code>_config.yml</code> to include
    either a <code>disqus.shortname</code> variable.
  </p>

  <p>
    If you do not wish to use Disqus, override the
    <code>comments.html</code> partial for this theme.
  </p>

  </section>
 -->
  <section class="related">
  <h2>Related Posts</h2>
  <ul class="posts-list">
    
      <li>
        <h3>
          <a href="/py/2020/06/11/Python-02-functions.html">
            Python functions
            <small>11 Jun 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/py/2020/06/11/Python-01-loops.html">
            Python Loops
            <small>11 Jun 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/py/2020/06/11/Python-00.html">
            Introduction to Python
            <small>11 Jun 2020</small>
          </a>
        </h3>
      </li>
    
  </ul>
</section>

</div>


    </main>

    <!-- Optional footer content -->

  </body>
</html>
